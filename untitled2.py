# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s_NWA5xcmdAQalrNBS51sdrXh8JO5UfR
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

data = pd.read_csv("/content/MNIST_train.csv")

data.head()

"""Data Transformation

"""

X= data.to_numpy()

X

y = X[:,2]

y

X = X[:,3:]

X

test= pd.read_csv("/content/MNIST_test.csv")

test= test.to_numpy()

X_test = test[:,3:]

y_test=test[:,2]

X.shape

"""Visualizations

"""

plt.figure()
plt.hist(y[y==1])
plt.hist(y[y==2])
plt.legend()

"""#  Naive Basesian Classifier assuming Normal Distribution"""

from scipy.stats import multivariate_normal as mvn
from sklearn.metrics import accuracy_score

class GaussNB():

  def fit(self, X, y, epsilon = 1e-3):
    self.likelihoods = dict()
    self.priors =dict()
    self.K =set(y.astype(int))

    for k in self.K:
      X_k = X[y==k]
      # Naive Assumption: Observations are linearly independent of each other
      self.likelihoods[k] ={"mean": X_k.mean(axis=0), "cov":X_k.var(axis=0)+epsilon}
      self.priors[k] = len(X_k)/len(X)

  def predict(self, X):
    N , D = X.shape
    P_hat = np.zeros((N,len(self.K)))

    for k , l in self.likelihoods.items():
      P_hat[:,k] = mvn.logpdf(X, l["mean"], l["cov"])+np.log(self.priors[k])

    return P_hat.argmax(axis=1)

X.shape

gaussB = GaussNB()

gaussB.fit(X,y)

pred=gaussB.predict(X_test)

pred

accuracy_score(pred,y_test)

"""# K Nearest Neighbors




"""

class KNNClassifier():
  def fit(self,X,y):
    self.X=X
    self.y=y

  def predict(self,X,k,epsilon=1e-3):
    N= len(X)
    y_hat = np.zeros(N)

    for i in range(N):
      dist2 = np.sum(((self.X - X[i])**2),axis=1)
      idxt = np.argsort(dist2)[:k]
      gamma_k = 1/(np.sqrt(dist2[idxt] + epsilon))
      y_hat[i]= np.bincount(self.y[idxt],weights=gamma_k).argmax()
    return y_hat

knn = KNNClassifier()

knn.fit(X,y)

X_test.shape

y_hat = knn.predict(X_test,10)

accuracy_score(y_hat,y_test)

"""NON-Naive classifier"""

class GaussBayes():
  def fit(self, X, y, epsilon=1e-3):
    self.likelihoods = dict()
    self.priors = dict()
    self.K = set(y.astype(int))
    for k in self.K:
      X_k = X[y==k,:]
      N_k, D = X_k.shape
      mu_k = X_k.mean(axis=0)
      self.likelihoods[k] = {"mean":mu_k, "cov":(1/(N_k-1))*np.matmul((X_k-mu_k).T,X_k-mu_k)+epsilon*np.identity(D)}
      self.priors[k] = len(X_k)/len(X)
  def predict(self, X):
    N, D = X.shape
    P_hat = np.zeros((N, len(self.K)))
    for k, l in self.likelihoods.items():
      P_hat[:,k] = mvn.logpdf(X, l["mean"], l["cov"])+np.log(self.priors[k])
    return P_hat.argmax(axis=1)

gaussB = GaussBayes()

gaussB.fit(X,y)

pred= gaussB.predict(X_test)

pred

accuracy_score(pred,y_test)